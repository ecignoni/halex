{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import gc\n",
    "\n",
    "import metatensor\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from halex.decomposition import EquivariantPCA\n",
    "from halex.models import RidgeOnEnergiesAndLowdinMultipleMolecules  # ByMO\n",
    "from halex.rotations import ClebschGordanReal\n",
    "from halex.train_utils import (\n",
    "    compute_features,\n",
    "    coupled_fock_matrix_from_multiple_molecules,\n",
    "    load_batched_dataset,\n",
    "    load_molecule_scf_datasets,\n",
    ")\n",
    "from halex.utils import drop_target_samples\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset for both STO-3G and def2-TZVP basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the frames, orbitals, Fock matrix, overlap matrix, orthogonalized Fock matrix and its block decomposition in `metatensor` format, eigenvalues and eigenvectors, and Löwdin charges in the minimal basis STO-3G and the fully converged def2-TZVP basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_molecule_datasets(base_folder:str, mol: str, cg: ClebschGordanReal, indices: np.ndarray):\n",
    "    \"\"\"\n",
    "    Load the SCFData objects storing data for a single molecule,\n",
    "    in both a small basis and a big basis\n",
    "    \"\"\"\n",
    "    coords_path = f\"{base_folder}/{mol}/coords_{mol}_1000.xyz\"\n",
    "    small_basis_path = f\"{base_folder}/{mol}/b3lyp_STO-3G/\"\n",
    "    big_basis_path = f\"{base_folder}/{mol}/b3lyp_def2tzvp/\"\n",
    "\n",
    "    sb_data, bb_data = load_molecule_scf_datasets(\n",
    "        coords_path=coords_path,\n",
    "        small_basis_path=small_basis_path,\n",
    "        big_basis_path=big_basis_path,\n",
    "        cg=cg,\n",
    "        train_indices=indices,\n",
    "    )\n",
    "\n",
    "    return sb_data, bb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = ClebschGordanReal(4)\n",
    "\n",
    "molecules = [\n",
    "    \"ethane\",\n",
    "    \"ethene\",\n",
    "    # \"butadiene\",\n",
    "    # \"hexane\",\n",
    "    # \"hexatriene\",\n",
    "    # \"isoprene\",\n",
    "    # \"styrene\",\n",
    "]\n",
    "\n",
    "np.random.seed(12345)\n",
    "indices = np.arange(1000)\n",
    "np.random.shuffle(indices)\n",
    "valid_indices = indices[-200:]\n",
    "indices = indices[:500]\n",
    "np.save(\"train_output/train_indices.npy\", indices)\n",
    "np.save(\"train_output/valid_indices.npy\", valid_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    mol: load_molecule_datasets(\"CH-dataset\", mol, cg=cg, indices=indices) for mol in molecules\n",
    "}\n",
    "\n",
    "valid_datasets = {\n",
    "    mol: load_molecule_datasets(\"CH-dataset\", mol, cg=cg, indices=valid_indices) for mol in molecules\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build features to learn the Hamiltonian we use the atom-centred equivariant representation $\\ket{\\overline{\\rho_i ^{\\otimes 2} ; \\sigma ; \\lambda \\mu}}$ to learn the orbital interactions on the same centre, $H_{ii}$ terms and the two-centred equivariant representation $\\ket{\\overline{\\rho_{ij} ^{\\otimes 1} ; \\sigma ; \\lambda \\mu}}$  to learn the the orbital interactions on two different centres, $H_{ij}$ terms. We have used the `librascal` library to compute these features and converted them to store in the `metatensor` format.\n",
    "\n",
    "We then reduce the dimensionality of these features using PCA and retain up to 200 principal components for each symmetry block.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:54<00:00, 57.11s/it]\n",
      "fitting PCA on each tensormap key: 13it [00:08,  1.54it/s]\n",
      "transforming each tensormap key: 13it [00:00, 20.81it/s]\n",
      "transforming each tensormap key: 13it [00:00, 49.44it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "transforming each tensormap key: 0it [00:00, ?it/s]\u001b[A\n",
      "transforming each tensormap key: 13it [00:00, 89.90it/s] \u001b[A\n",
      " 50%|█████     | 1/2 [00:26<00:26, 26.49s/it]\n",
      "transforming each tensormap key: 13it [00:00, 156.61it/s]\n",
      "100%|██████████| 2/2 [00:38<00:00, 19.26s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6377"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rascal_hypers = {\n",
    "    \"interaction_cutoff\": 3.5,\n",
    "    \"cutoff_smooth_width\": 0.5,\n",
    "    \"max_radial\": 6,\n",
    "    \"max_angular\": 4,\n",
    "    \"gaussian_sigma_constant\": 0.2,\n",
    "    \"gaussian_sigma_type\": \"Constant\",\n",
    "    \"compute_gradients\": False,\n",
    "}\n",
    "\n",
    "feats = compute_features(datasets, rascal_hypers=rascal_hypers, cg=cg, lcut=2)\n",
    "gc.collect()\n",
    "\n",
    "epca = EquivariantPCA(n_components=200).fit(metatensor.join(feats, axis=\"samples\"))\n",
    "\n",
    "feats = [epca.transform(feats_) for feats_ in feats]\n",
    "gc.collect()\n",
    "\n",
    "epca.save(\"train_output/epca.npz\")\n",
    "\n",
    "valid_feats = compute_features(\n",
    "    valid_datasets, rascal_hypers=rascal_hypers, cg=cg, lcut=2, epca=epca\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batched dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nelec_dict = {\"H\": 1.0, \"C\": 6.0}\n",
    "\n",
    "multimol_datasets = [\n",
    "    load_batched_dataset(\n",
    "        scf_datasets=data,\n",
    "        feats=feat,\n",
    "        nelec_dict=nelec_dict,\n",
    "        batch_size=100,\n",
    "        lowdin_charges_by_MO=False,\n",
    "        # lowdin_mo_indices=indices,\n",
    "    )\n",
    "    for data, feat in zip(datasets.values(), feats)\n",
    "]\n",
    "\n",
    "valid_multimol_datasets = [\n",
    "    load_batched_dataset(\n",
    "        scf_datasets=data,\n",
    "        feats=feat,\n",
    "        nelec_dict=nelec_dict,\n",
    "        batch_size=50,\n",
    "        lowdin_charges_by_MO=False,\n",
    "        # lowdin_mo_indices=indices,\n",
    "    )\n",
    "    for data, feat in zip(valid_datasets.values(), valid_feats)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Change to the Coupled basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the block decomposed Hamiltonian to a coupled basis and drop samples that are not present in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all keys matched successfully\n"
     ]
    }
   ],
   "source": [
    "targ_coupled = coupled_fock_matrix_from_multiple_molecules(datasets.values())\n",
    "targ_coupled = drop_target_samples(\n",
    "    metatensor.join(feats, axis=\"samples\"), targ_coupled, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a symmetry adapted model of the minimal basis (STO-3G) Hamiltonian and train it against a fully converged def2-TZVP basis, using the eigenvalues and partial charges as indirect targets. The model is initialized from the weights obtained by the analytical solution of the ridge regression model (symmetry adapted) that uses as target the minimal basis Hamiltonian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RidgeOnEnergiesAndLowdinMultipleMolecules(\n",
    "    coupled_tmap=targ_coupled,\n",
    "    features=metatensor.join(feats, axis=\"samples\"),\n",
    "    alpha=1e-14,\n",
    "    dump_dir=\"train_output\",\n",
    "    bias=False,\n",
    ")\n",
    "\n",
    "model.fit_ridge_analytical(\n",
    "    features=metatensor.join(feats, axis=\"samples\"),\n",
    "    targets=targ_coupled,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                   | 28/20000 [01:25<16:59:07,  3.06s/it, train_total=9.46e+4, valid_total=9.44e+4]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-416b3646e4d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_datasets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultimol_datasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalid_datasets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_multimol_datasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20_000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptim_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lavoro/source/halex/halex/models/wrappers.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_datasets, valid_datasets, epochs, optim_kwargs, verbose, dump, loss_kwargs)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalid_datasets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m                 \u001b[0mvalid_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/lib/python3.8/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lavoro/source/halex/halex/models/wrappers.py\u001b[0m in \u001b[0;36m_validation_step\u001b[0;34m(self, valid_datasets, loss_kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_core\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m                 loss, *other_losses = self.loss_fn(\n\u001b[0m\u001b[1;32m    684\u001b[0m                     \u001b[0mpred_blocks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m                     \u001b[0mframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lavoro/source/halex/halex/models/wrappers.py\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(self, pred_blocks, frames, eigvals, lowdinq, orbs, ao_labels, nelec_dict, weight_eigvals, weight_lowdinq, weight_regloss, **kwargs)\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n\u001b[0;32m--> 613\u001b[0;31m         return _loss_eigenvalues_lowdinq_vectorized(\n\u001b[0m\u001b[1;32m    614\u001b[0m             \u001b[0mpred_blocks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_blocks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0mframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lavoro/source/halex/halex/models/wrappers.py\u001b[0m in \u001b[0;36m_loss_eigenvalues_lowdinq_vectorized\u001b[0;34m(pred_blocks, frames, eigvals, lowdinq, orbs, ao_labels, nelec_dict, regloss, weight_eigvals, weight_lowdinq, weight_regloss, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \"\"\"\n\u001b[1;32m    130\u001b[0m     \u001b[0;31m# predict fock matrices as torch.Tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mpred_focks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_predict_focks_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# MO energies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lavoro/source/halex/halex/models/wrappers.py\u001b[0m in \u001b[0;36m_predict_focks_vectorized\u001b[0;34m(pred_blocks, frames, orbs)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0msame\u001b[0m \u001b[0mdimension\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0mmolecule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mblocks_to_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecouple_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lavoro/source/halex/halex/hamiltonian/tensormap_dense.py\u001b[0m in \u001b[0;36mblocks_to_dense\u001b[0;34m(blocks, frames, orbs, vectorized)\u001b[0m\n\u001b[1;32m    196\u001b[0m ) -> Union[np.ndarray, torch.Tensor]:\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvectorized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_vectorized_blocks_to_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_blocks_to_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lavoro/source/halex/halex/hamiltonian/tensormap_dense.py\u001b[0m in \u001b[0;36m_vectorized_blocks_to_dense\u001b[0;34m(blocks, frames, orbs)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;31m# this is the computationally expensive part of the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# we use caching to speed up the process a bit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         fslices, islices, jslices, islices2, jslices2 = _get_slices(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mcur_A\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_A\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lavoro/source/halex/halex/hamiltonian/tensormap_dense.py\u001b[0m in \u001b[0;36m_get_slices\u001b[0;34m(block, cur_A, dense_idx, atom_blocks_idx, ki_offset, kj_offset, li, lj, idx, slices_cache)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0mjslices2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;31m# loops over samples (structure, i, j)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# check if we have to update the frame and index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcur_A\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_datasets=multimol_datasets,\n",
    "    valid_datasets=valid_multimol_datasets,\n",
    "    epochs=20_000,\n",
    "    optim_kwargs=dict(lr=1),\n",
    "    verbose=10,\n",
    "    dump=50,\n",
    ")\n",
    "\n",
    "model.dump_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
