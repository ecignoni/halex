{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import gc\n",
    "\n",
    "import metatensor\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from halex.decomposition import EquivariantPCA\n",
    "from halex.models import RidgeOnEnergiesAndLowdinMultipleMolecules  # ByMO\n",
    "from halex.rotations import ClebschGordanReal\n",
    "from halex.train_utils import (\n",
    "    compute_features,\n",
    "    coupled_fock_matrix_from_multiple_molecules,\n",
    "    load_batched_dataset,\n",
    "    load_molecule_scf_datasets,\n",
    ")\n",
    "from halex.utils import drop_target_samples\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset for both STO-3G and def2-TZVP basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the frames, orbitals, Fock matrix, overlap matrix, orthogonalized Fock matrix and its block decomposition in `metatensor` format, eigenvalues and eigenvectors, and Löwdin charges in the minimal basis STO-3G and the fully converged def2-TZVP basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_molecule_datasets(base_folder:str, mol: str, cg: ClebschGordanReal, indices: np.ndarray):\n",
    "    \"\"\"\n",
    "    Load the SCFData objects storing data for a single molecule,\n",
    "    in both a small basis and a big basis\n",
    "    \"\"\"\n",
    "    coords_path = f\"{base_folder}/{mol}/coords_{mol}_1000.xyz\"\n",
    "    small_basis_path = f\"{base_folder}/{mol}/b3lyp_STO-3G/\"\n",
    "    big_basis_path = f\"{base_folder}/{mol}/b3lyp_def2tzvp/\"\n",
    "\n",
    "    sb_data, bb_data = load_molecule_scf_datasets(\n",
    "        coords_path=coords_path,\n",
    "        small_basis_path=small_basis_path,\n",
    "        big_basis_path=big_basis_path,\n",
    "        cg=cg,\n",
    "        train_indices=indices,\n",
    "    )\n",
    "\n",
    "    return sb_data, bb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = ClebschGordanReal(4)\n",
    "\n",
    "molecules = [\n",
    "    \"ethane\",\n",
    "    \"ethene\",\n",
    "    # \"butadiene\",\n",
    "    # \"hexane\",\n",
    "    # \"hexatriene\",\n",
    "    # \"isoprene\",\n",
    "    # \"styrene\",\n",
    "]\n",
    "\n",
    "np.random.seed(12345)\n",
    "indices = np.arange(1000)\n",
    "np.random.shuffle(indices)\n",
    "valid_indices = indices[-200:]\n",
    "indices = indices[:500]\n",
    "np.save(\"train_output/train_indices.npy\", indices)\n",
    "np.save(\"train_output/valid_indices.npy\", valid_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    mol: load_molecule_datasets(\"CH-dataset\", mol, cg=cg, indices=indices) for mol in molecules\n",
    "}\n",
    "\n",
    "valid_datasets = {\n",
    "    mol: load_molecule_datasets(\"CH-dataset\", mol, cg=cg, indices=valid_indices) for mol in molecules\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build features to learn the Hamiltonian we use the atom-centred equivariant representation $\\ket{\\overline{\\rho_i ^{\\otimes 2} ; \\sigma ; \\lambda \\mu}}$ to learn the orbital interactions on the same centre, $H_{ii}$ terms and the two-centred equivariant representation $\\ket{\\overline{\\rho_{ij} ^{\\otimes 1} ; \\sigma ; \\lambda \\mu}}$  to learn the the orbital interactions on two different centres, $H_{ij}$ terms. We have used the `librascal` library to compute these features and converted them to store in the `metatensor` format.\n",
    "\n",
    "We then reduce the dimensionality of these features using PCA and retain up to 200 principal components for each symmetry block.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suman/miniconda3/envs/mlelec/lib/python3.10/site-packages/rascaline/systems/ase.py:57: UserWarning: periodic boundary conditions are disabled, but the cell matrix is not zero, we will set the cell to zero.\n",
      "  warnings.warn(\n",
      "100%|██████████| 2/2 [01:12<00:00, 36.49s/it]\n",
      "fitting PCA on each tensormap key: 13it [00:06,  1.93it/s]\n",
      "transforming each tensormap key: 13it [00:00, 67.34it/s]\n",
      "transforming each tensormap key: 13it [00:00, 54.94it/s]\n",
      "transforming each tensormap key: 13it [00:00, 146.19it/s]\n",
      "transforming each tensormap key: 13it [00:00, 191.62it/s]\n",
      "100%|██████████| 2/2 [00:26<00:00, 13.30s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5375"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypers = {\n",
    "    \"cutoff\": 3.5,\n",
    "    \"max_radial\": 6,\n",
    "    \"max_angular\": 4,\n",
    "    \"atomic_gaussian_width\": 0.2,\n",
    "    \"radial_basis\": {\"Gto\": {}},\n",
    "    \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}},\n",
    "    \"center_atom_weight\": 1.0,\n",
    "}\n",
    "\n",
    "feats = compute_features(datasets, hypers, cg=cg, lcut=2)\n",
    "gc.collect()\n",
    "\n",
    "epca = EquivariantPCA(n_components=200).fit(metatensor.join(feats, axis=\"samples\"))\n",
    "\n",
    "feats = [epca.transform(feats_) for feats_ in feats]\n",
    "gc.collect()\n",
    "\n",
    "epca.save(\"train_output/epca.npz\")\n",
    "\n",
    "valid_feats = compute_features(\n",
    "    valid_datasets, hypers, cg=cg, lcut=2, epca=epca\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batched dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nelec_dict = {\"H\": 1.0, \"C\": 6.0}\n",
    "\n",
    "multimol_datasets = [\n",
    "    load_batched_dataset(\n",
    "        scf_datasets=data,\n",
    "        feats=feat,\n",
    "        nelec_dict=nelec_dict,\n",
    "        batch_size=100,\n",
    "        lowdin_charges_by_MO=False,\n",
    "        # lowdin_mo_indices=indices,\n",
    "    )\n",
    "    for data, feat in zip(datasets.values(), feats)\n",
    "]\n",
    "\n",
    "valid_multimol_datasets = [\n",
    "    load_batched_dataset(\n",
    "        scf_datasets=data,\n",
    "        feats=feat,\n",
    "        nelec_dict=nelec_dict,\n",
    "        batch_size=50,\n",
    "        lowdin_charges_by_MO=False,\n",
    "        # lowdin_mo_indices=indices,\n",
    "    )\n",
    "    for data, feat in zip(valid_datasets.values(), valid_feats)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Change to the Coupled basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the block decomposed Hamiltonian to a coupled basis and drop samples that are not present in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all keys matched successfully\n"
     ]
    }
   ],
   "source": [
    "targ_coupled = coupled_fock_matrix_from_multiple_molecules(datasets.values())\n",
    "targ_coupled = drop_target_samples(\n",
    "    metatensor.join(feats, axis=\"samples\"), targ_coupled, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a symmetry adapted model of the minimal basis (STO-3G) Hamiltonian and train it against a fully converged def2-TZVP basis, using the eigenvalues and partial charges as indirect targets. The model is initialized from the weights obtained by the analytical solution of the ridge regression model (symmetry adapted) that uses as target the minimal basis Hamiltonian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RidgeOnEnergiesAndLowdinMultipleMolecules(\n",
    "    coupled_tmap=targ_coupled,\n",
    "    features=metatensor.join(feats, axis=\"samples\"),\n",
    "    alpha=1e-14,\n",
    "    dump_dir=\"train_output\",\n",
    "    bias=False,\n",
    ")\n",
    "\n",
    "model.fit_ridge_analytical(\n",
    "    features=metatensor.join(feats, axis=\"samples\"),\n",
    "    targets=targ_coupled,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                 | 480/20000 [19:10<13:00:04,  2.40s/it, train_total=7.36e+5, valid_total=5.43e+5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/scratch/suman/halex_rascaline/halex/train.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/scratch/suman/halex_rascaline/halex/train.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell:/scratch/suman/halex_rascaline/halex/train.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train_datasets\u001b[39m=\u001b[39;49mmultimol_datasets,\n\u001b[1;32m      <a href='vscode-notebook-cell:/scratch/suman/halex_rascaline/halex/train.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     valid_datasets\u001b[39m=\u001b[39;49mvalid_multimol_datasets,\n\u001b[1;32m      <a href='vscode-notebook-cell:/scratch/suman/halex_rascaline/halex/train.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m20_000\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/scratch/suman/halex_rascaline/halex/train.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     optim_kwargs\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(lr\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/scratch/suman/halex_rascaline/halex/train.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/scratch/suman/halex_rascaline/halex/train.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     dump\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/scratch/suman/halex_rascaline/halex/train.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/scratch/suman/halex_rascaline/halex/train.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39mdump_state()\n",
      "File \u001b[0;32m/scratch/suman/halex_rascaline/halex/halex/models/wrappers.py:728\u001b[0m, in \u001b[0;36mRidgeOnEnergiesAndLowdinMultipleMolecules.fit\u001b[0;34m(self, train_datasets, valid_datasets, epochs, optim_kwargs, verbose, dump, loss_kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m iterator \u001b[39m=\u001b[39m tqdm(\u001b[39mrange\u001b[39m(epochs), ncols\u001b[39m=\u001b[39m\u001b[39m120\u001b[39m)\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m iterator:\n\u001b[0;32m--> 728\u001b[0m     losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_step(train_datasets, optimizer, loss_kwargs)\n\u001b[1;32m    730\u001b[0m     \u001b[39mif\u001b[39;00m valid_datasets \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m         valid_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validation_step(valid_datasets, loss_kwargs)\n",
      "File \u001b[0;32m/scratch/suman/halex_rascaline/halex/halex/models/wrappers.py:645\u001b[0m, in \u001b[0;36mRidgeOnEnergiesAndLowdinMultipleMolecules._train_step\u001b[0;34m(self, train_datasets, optimizer, loss_kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    643\u001b[0m pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(features\u001b[39m=\u001b[39mx, core_features\u001b[39m=\u001b[39mx_core)\n\u001b[0;32m--> 645\u001b[0m loss, \u001b[39m*\u001b[39mother_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_fn(\n\u001b[1;32m    646\u001b[0m     pred_blocks\u001b[39m=\u001b[39;49mpred,\n\u001b[1;32m    647\u001b[0m     frames\u001b[39m=\u001b[39;49mframes,\n\u001b[1;32m    648\u001b[0m     eigvals\u001b[39m=\u001b[39;49meigvals,\n\u001b[1;32m    649\u001b[0m     lowdinq\u001b[39m=\u001b[39;49mlowdinq,\n\u001b[1;32m    650\u001b[0m     orbs\u001b[39m=\u001b[39;49mtrain_dataset\u001b[39m.\u001b[39;49morbs,\n\u001b[1;32m    651\u001b[0m     ao_labels\u001b[39m=\u001b[39;49mtrain_dataset\u001b[39m.\u001b[39;49mao_labels,\n\u001b[1;32m    652\u001b[0m     nelec_dict\u001b[39m=\u001b[39;49mtrain_dataset\u001b[39m.\u001b[39;49mnelec_dict,\n\u001b[1;32m    653\u001b[0m     mo_indices\u001b[39m=\u001b[39;49mtrain_dataset\u001b[39m.\u001b[39;49mlowdin_mo_indices,\n\u001b[1;32m    654\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mloss_kwargs,\n\u001b[1;32m    655\u001b[0m )\n\u001b[1;32m    657\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    658\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/scratch/suman/halex_rascaline/halex/halex/models/wrappers.py:613\u001b[0m, in \u001b[0;36mRidgeOnEnergiesAndLowdinMultipleMolecules.loss_fn\u001b[0;34m(self, pred_blocks, frames, eigvals, lowdinq, orbs, ao_labels, nelec_dict, weight_eigvals, weight_lowdinq, weight_regloss, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_fn\u001b[39m(\n\u001b[1;32m    600\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    601\u001b[0m     pred_blocks: TensorMap,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    612\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 613\u001b[0m     \u001b[39mreturn\u001b[39;00m _loss_eigenvalues_lowdinq_vectorized(\n\u001b[1;32m    614\u001b[0m         pred_blocks\u001b[39m=\u001b[39;49mpred_blocks,\n\u001b[1;32m    615\u001b[0m         frames\u001b[39m=\u001b[39;49mframes,\n\u001b[1;32m    616\u001b[0m         eigvals\u001b[39m=\u001b[39;49meigvals,\n\u001b[1;32m    617\u001b[0m         lowdinq\u001b[39m=\u001b[39;49mlowdinq,\n\u001b[1;32m    618\u001b[0m         orbs\u001b[39m=\u001b[39;49morbs,\n\u001b[1;32m    619\u001b[0m         ao_labels\u001b[39m=\u001b[39;49mao_labels,\n\u001b[1;32m    620\u001b[0m         nelec_dict\u001b[39m=\u001b[39;49mnelec_dict,\n\u001b[1;32m    621\u001b[0m         regloss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mregloss_,\n\u001b[1;32m    622\u001b[0m         weight_eigvals\u001b[39m=\u001b[39;49mweight_eigvals,\n\u001b[1;32m    623\u001b[0m         weight_lowdinq\u001b[39m=\u001b[39;49mweight_lowdinq,\n\u001b[1;32m    624\u001b[0m         weight_regloss\u001b[39m=\u001b[39;49mweight_regloss,\n\u001b[1;32m    625\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    626\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/suman/halex_rascaline/halex/halex/models/wrappers.py:131\u001b[0m, in \u001b[0;36m_loss_eigenvalues_lowdinq_vectorized\u001b[0;34m(pred_blocks, frames, eigvals, lowdinq, orbs, ao_labels, nelec_dict, regloss, weight_eigvals, weight_lowdinq, weight_regloss, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"combined loss on MO energies and Lowdin charges\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[39mComputes a (regularized) combined loss on MO energies and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mthat have the same dimension (e.g., for a single molecule).\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m# predict fock matrices as torch.Tensors\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m pred_focks \u001b[39m=\u001b[39m _predict_focks_vectorized(pred_blocks, frames\u001b[39m=\u001b[39;49mframes, orbs\u001b[39m=\u001b[39;49morbs)\n\u001b[1;32m    133\u001b[0m \u001b[39m# MO energies\u001b[39;00m\n\u001b[1;32m    134\u001b[0m pred_eigvals \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39meigvalsh(pred_focks)\n",
      "File \u001b[0;32m/scratch/suman/halex_rascaline/halex/halex/models/wrappers.py:29\u001b[0m, in \u001b[0;36m_predict_focks_vectorized\u001b[0;34m(pred_blocks, frames, orbs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict_focks_vectorized\u001b[39m(pred_blocks, frames, orbs):\n\u001b[1;32m     23\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Converts the block tensormap back to dense fock matrices\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[39m    This is a vectorized (faster evaluation, faster backpropagation)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m    version of the prediction. Only works if all the focks have the\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m    same dimension (e.g., for a single molecule)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[39mreturn\u001b[39;00m blocks_to_dense(decouple_blocks(pred_blocks), frames, orbs, vectorized\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/scratch/suman/halex_rascaline/halex/halex/hamiltonian/tensormap_dense.py:198\u001b[0m, in \u001b[0;36mblocks_to_dense\u001b[0;34m(blocks, frames, orbs, vectorized)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mblocks_to_dense\u001b[39m(\n\u001b[1;32m    192\u001b[0m     blocks: TensorMap,\n\u001b[1;32m    193\u001b[0m     frames: List[Atoms],\n\u001b[1;32m    194\u001b[0m     orbs: Dict[\u001b[39mint\u001b[39m, List[Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m]]],\n\u001b[1;32m    195\u001b[0m     vectorized: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    196\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[np\u001b[39m.\u001b[39mndarray, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    197\u001b[0m     \u001b[39mif\u001b[39;00m vectorized:\n\u001b[0;32m--> 198\u001b[0m         \u001b[39mreturn\u001b[39;00m _vectorized_blocks_to_dense(blocks\u001b[39m=\u001b[39;49mblocks, frames\u001b[39m=\u001b[39;49mframes, orbs\u001b[39m=\u001b[39;49morbs)\n\u001b[1;32m    199\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m         \u001b[39mreturn\u001b[39;00m _blocks_to_dense(blocks\u001b[39m=\u001b[39mblocks, frames\u001b[39m=\u001b[39mframes, orbs\u001b[39m=\u001b[39morbs)\n",
      "File \u001b[0;32m/scratch/suman/halex_rascaline/halex/halex/hamiltonian/tensormap_dense.py:403\u001b[0m, in \u001b[0;36m_vectorized_blocks_to_dense\u001b[0;34m(blocks, frames, orbs)\u001b[0m\n\u001b[1;32m    397\u001b[0m same_koff \u001b[39m=\u001b[39m ki_offset \u001b[39m==\u001b[39m kj_offset\n\u001b[1;32m    399\u001b[0m \u001b[39m# get the slices of the dense hamiltonian in order to assign\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[39m# the values from the tensormap\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39m# this is the computationally expensive part of the function\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39m# we use caching to speed up the process a bit\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m fslices, islices, jslices, islices2, jslices2 \u001b[39m=\u001b[39m _get_slices(\n\u001b[1;32m    404\u001b[0m     block\u001b[39m=\u001b[39;49mblock,\n\u001b[1;32m    405\u001b[0m     cur_A\u001b[39m=\u001b[39;49mcur_A,\n\u001b[1;32m    406\u001b[0m     dense_idx\u001b[39m=\u001b[39;49mdense_idx,\n\u001b[1;32m    407\u001b[0m     atom_blocks_idx\u001b[39m=\u001b[39;49matom_blocks_idx,\n\u001b[1;32m    408\u001b[0m     ki_offset\u001b[39m=\u001b[39;49mki_offset,\n\u001b[1;32m    409\u001b[0m     kj_offset\u001b[39m=\u001b[39;49mkj_offset,\n\u001b[1;32m    410\u001b[0m     li\u001b[39m=\u001b[39;49mli,\n\u001b[1;32m    411\u001b[0m     lj\u001b[39m=\u001b[39;49mlj,\n\u001b[1;32m    412\u001b[0m     idx\u001b[39m=\u001b[39;49midx,\n\u001b[1;32m    413\u001b[0m     slices_cache\u001b[39m=\u001b[39;49mSLICES_CACHE,\n\u001b[1;32m    414\u001b[0m )\n\u001b[1;32m    416\u001b[0m \u001b[39mif\u001b[39;00m block_type \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    417\u001b[0m     values \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtranspose(block\u001b[39m.\u001b[39mvalues, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/scratch/suman/halex_rascaline/halex/halex/hamiltonian/tensormap_dense.py:475\u001b[0m, in \u001b[0;36m_get_slices\u001b[0;34m(block, cur_A, dense_idx, atom_blocks_idx, ki_offset, kj_offset, li, lj, idx, slices_cache)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[39m# coordinates of the atom block in the matrix\u001b[39;00m\n\u001b[1;32m    473\u001b[0m ki_base, kj_base \u001b[39m=\u001b[39m atom_blocks_idx[(dense_idx, i, j)]\n\u001b[0;32m--> 475\u001b[0m fslice, islice, jslice, islice2, jslice2 \u001b[39m=\u001b[39m _get_slices_cached(\n\u001b[1;32m    476\u001b[0m     ki_base\u001b[39m=\u001b[39;49mki_base,\n\u001b[1;32m    477\u001b[0m     kj_base\u001b[39m=\u001b[39;49mkj_base,\n\u001b[1;32m    478\u001b[0m     ki_offset\u001b[39m=\u001b[39;49mki_offset,\n\u001b[1;32m    479\u001b[0m     kj_offset\u001b[39m=\u001b[39;49mkj_offset,\n\u001b[1;32m    480\u001b[0m     li\u001b[39m=\u001b[39;49mli,\n\u001b[1;32m    481\u001b[0m     lj\u001b[39m=\u001b[39;49mlj,\n\u001b[1;32m    482\u001b[0m     dense_idx\u001b[39m=\u001b[39;49mdense_idx,\n\u001b[1;32m    483\u001b[0m     block_idx\u001b[39m=\u001b[39;49midx,\n\u001b[1;32m    484\u001b[0m     slices_cache\u001b[39m=\u001b[39;49mslices_cache,\n\u001b[1;32m    485\u001b[0m )\n\u001b[1;32m    487\u001b[0m islices\u001b[39m.\u001b[39mappend(islice)\n\u001b[1;32m    488\u001b[0m jslices\u001b[39m.\u001b[39mappend(jslice)\n",
      "File \u001b[0;32m/scratch/suman/halex_rascaline/halex/halex/hamiltonian/tensormap_dense.py:502\u001b[0m, in \u001b[0;36m_get_slices_cached\u001b[0;34m(ki_base, kj_base, ki_offset, kj_offset, li, lj, dense_idx, block_idx, slices_cache)\u001b[0m\n\u001b[1;32m    497\u001b[0m     jslices2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(jslices2)\n\u001b[1;32m    499\u001b[0m     \u001b[39mreturn\u001b[39;00m fslices, islices, jslices, islices2, jslices2\n\u001b[0;32m--> 502\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_slices_cached\u001b[39m(\n\u001b[1;32m    503\u001b[0m     ki_base: \u001b[39mint\u001b[39m,\n\u001b[1;32m    504\u001b[0m     kj_base: \u001b[39mint\u001b[39m,\n\u001b[1;32m    505\u001b[0m     ki_offset: \u001b[39mint\u001b[39m,\n\u001b[1;32m    506\u001b[0m     kj_offset: \u001b[39mint\u001b[39m,\n\u001b[1;32m    507\u001b[0m     li: \u001b[39mint\u001b[39m,\n\u001b[1;32m    508\u001b[0m     lj: \u001b[39mint\u001b[39m,\n\u001b[1;32m    509\u001b[0m     dense_idx: \u001b[39mint\u001b[39m,\n\u001b[1;32m    510\u001b[0m     block_idx: Labels,\n\u001b[1;32m    511\u001b[0m     slices_cache: Dict,\n\u001b[1;32m    512\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mndarray]:\n\u001b[1;32m    513\u001b[0m     key \u001b[39m=\u001b[39m (ki_base, kj_base, ki_offset, kj_offset, li, lj, \u001b[39mtuple\u001b[39m(block_idx))\n\u001b[1;32m    514\u001b[0m     \u001b[39m# try to get the cached slices\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_datasets=multimol_datasets,\n",
    "    valid_datasets=valid_multimol_datasets,\n",
    "    epochs=20_000,\n",
    "    optim_kwargs=dict(lr=1),\n",
    "    verbose=10,\n",
    "    dump=50,\n",
    ")\n",
    "\n",
    "model.dump_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
